data_root: /nfs/scratch/dmamontov/daicwoz
meta_csv: /nfs/scratch/dmamontov/daicwoz/meta_info.csv

work_root: /nfs/scratch/jtan/daic_runs
participant_wavs_dir: ${work_root}/participant_wavs
segments_index_csv: ${work_root}/segments_index.csv
sr: 16000
win_sec: 10.0
hop_sec: 5.0

models:
  wavlm_large_embed:
    backend: hf_audio_embedding
    ckpt: microsoft/wavlm-large
    layer: -1              # Extract features from the last layer (configurable)
    pooling: mean          # Apply mean/attention pooling to frames to generate segment vectors

  wav2vec2_xlsr_embed:
    backend: hf_audio_embedding
    ckpt: facebook/wav2vec2-large-xlsr-53
    layer: -1
    pooling: mean

  wav2vec2_audeering_emotion_embed:
    backend: hf_audio_embedding
    ckpt: audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim
    take: hidden_states     # Only use hidden layers, without the emotion regression head
    layer: -1
    pooling: mean

  hubert_large_embed:
    backend: hf_audio_embedding
    ckpt: facebook/hubert-large-ll60k
    layer: -1
    pooling: mean

  # Keep Whisper but only use as embedding (not recommended, ASR is more suitable)
  whisper_large_embed:
    backend: hf_audio_embedding
    ckpt: openai/whisper-large-v3
    feature_type: encoder  # Extract encoder output

  roberta_large_text:
    backend: hf_text_classify
    ckpt: rafalposwiata/roberta-large-depression
    target_label: depressed
 
